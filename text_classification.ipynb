{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKzgZCaD9tX8",
        "outputId": "a3e51ee9-a93e-450a-b8db-86b802efc207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package mock_corpus to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mock_corpus.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = {\n",
        "    \"finance\": [\n",
        "        \"stock\", \"market\", \"investment\", \"economy\", \"bank\", \"loan\", \"interest rate\",\n",
        "        \"inflation\", \"currency\", \"profit\", \"loss\", \"revenue\", \"expenditure\",\n",
        "        \"dividend\", \"portfolio\", \"trade\", \"share\", \"bond\", \"equity\", \"financial\",\n",
        "        \"gdp\", \"recession\", \"fund\", \"asset\", \"liability\", \"tax\", \"budget\", \"audit\",\n",
        "        \"venture capital\", \"ipo\", \"merger\", \"acquisition\", \"debt\", \"credit\", \"fiscal\",\n",
        "        \"earning\", \"forecast\", \"yield\", \"commodity\", \"derivative\", \"hedging\",\n",
        "        \"exchange\", \"liquidity\", \"capital\", \"underwriting\", \"broker\", \"analyst\"\n",
        "    ],\n",
        "    \"medical\": [\n",
        "        \"patient\", \"doctor\", \"hospital\", \"disease\", \"treatment\", \"diagnosis\", \"symptom\",\n",
        "        \"medication\", \"surgery\", \"therapy\", \"clinic\", \"health\", \"illness\", \"virus\",\n",
        "        \"bacteria\", \"vaccine\", \"research\", \"clinical trial\", \"anatomy\", \"physiology\",\n",
        "        \"pharmacology\", \"epidemic\", \"pandemic\", \"prescription\", \"drug\", \"mri\", \"x-ray\",\n",
        "        \"rehabilitation\", \"nursing\", \"medical device\", \"consultation\", \"prevention\",\n",
        "        \"wellness\", \"pathology\", \"immunology\", \"neurology\", \"cardiology\", \"oncology\",\n",
        "        \"pediatrics\", \"geriatrics\", \"diagnosis\", \"prognosis\", \"therapy\", \"biotechnology\",\n",
        "        \"medical record\", \"health care\", \"clinical study\", \"pharma\"\n",
        "    ],\n",
        "    \"technology\": [\n",
        "        \"software\", \"hardware\", \"computer\", \"internet\", \"network\", \"data\", \"ai\",\n",
        "        \"machine learning\", \"algorithm\", \"cybersecurity\", \"development\", \"programming\",\n",
        "        \"code\", \"app\", \"website\", \"cloud computing\", \"startup\", \"innovation\", \"digital\",\n",
        "        \"robotics\", \"automation\", \"gadget\", \"smartphone\", \"blockchain\", \"virtual reality\",\n",
        "        \"augmented reality\", \"chip\", \"processor\", \"server\", \"database\", \"api\", \"ux\", \"ui\",\n",
        "        \"firmware\", \"protocol\", \"bandwidth\", \"encryption\", \"biometric\", \"nanotechnology\",\n",
        "        \"telecom\", \"gigabit\", \"ethernet\", \"analytic\", \"big data\", \"coding\", \"developer\",\n",
        "        \"neural network\", \"artificial intelligence\", \"tech company\", \"operating system\"\n",
        "    ],\n",
        "    \"sports\": [\n",
        "        \"game\", \"match\", \"team\", \"player\", \"score\", \"win\", \"lose\", \"championship\", \"league\",\n",
        "        \"athlete\", \"coach\", \"stadium\", \"tournament\", \"goal\", \"point\", \"record\", \"training\",\n",
        "        \"fitness\", \"olympic\", \"world cup\", \"basketball\", \"football\", \"cricket\", \"tennis\",\n",
        "        \"swimming\", \"runner\", \"umpire\", \"referee\", \"medal\", \"cup\", \"season\", \"pitch\", \"court\",\n",
        "        \"field\", \"race\", \"compete\", \"victory\", \"defeat\", \"champion\", \"trophy\", \"fixture\",\n",
        "        \"event\", \"fan\", \"supporter\", \"league table\", \"transfer\", \"sporting event\", \"playoff\", \"tournament\"\n",
        "    ],\n",
        "    \"military\": [\n",
        "        \"army\", \"navy\", \"air force\", \"soldier\", \"weapon\", \"combat\", \"war\", \"defense\",\n",
        "        \"security\", \"military operation\", \"troop\", \"battle\", \"intelligence\", \"commander\",\n",
        "        \"strategy\", \"tactics\", \"deployment\", \"veteran\", \"peacekeeping\", \"ammunition\",\n",
        "        \"artillery\", \"infantry\", \"navy seal\", \"air strike\", \"missile\", \"uniform\", \"drill\",\n",
        "        \"sanction\", \"bunker\", \"grenade\", \"camouflage\", \"reconnaissance\", \"logistics\",\n",
        "        \"fortification\", \"cadet\", \"serviceman\", \"patrol\", \"conflict\", \"frontline\",\n",
        "        \"regiment\", \"battalion\", \"squadron\", \"fleet\", \"armored\", \"counter-terrorism\",\n",
        "        \"military base\", \"armed force\", \"national guard\", \"servicemember\"\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "ooepchNU9yRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strong phrases (multi-word terms that are strong indicators)\n",
        "strong_phrases = {\n",
        "    \"finance\": [\"stock market\", \"interest rate hike\", \"quarterly earnings\", \"financial crisis\", \"venture capital\", \"fiscal policy\", \"monetary policy\", \"investment bank\", \"credit rating\", \"economic growth\"],\n",
        "    \"medical\": [\"clinical trial results\", \"public health emergency\", \"disease outbreak\", \"medical breakthrough\", \"patient care\", \"emergency room\", \"side effects\", \"health care system\", \"surgical procedure\", \"drug discovery\"],\n",
        "    \"technology\": [\"artificial intelligence\", \"machine learning\", \"cyber security threat\", \"software development\", \"cloud computing\", \"virtual reality\", \"augmented reality\", \"data science\", \"internet of things\", \"quantum computing\", \"neural network\"],\n",
        "    \"sports\": [\"world cup final\", \"championship game\", \"team victory\", \"olympic medal\", \"record breaking\", \"league leader\", \"playoff series\", \"final score\", \"athlete performance\", \"sporting event\"],\n",
        "    \"military\": [\"military exercise\", \"national security\", \"combat operations\", \"defense budget\", \"peacekeeping mission\", \"armed forces\", \"intelligence gathering\", \"strategic deployment\", \"air superiority\", \"military alliance\", \"special forces\"]\n",
        "}"
      ],
      "metadata": {
        "id": "NSk8uDjPCCxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "4OhHInrKCDW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "J6iY-NPSCTqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('A'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "metadata": {
        "id": "elMC14U-CzFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text_lower = text.lower()\n",
        "  tokens = word_tokenize(text_lower)\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "  tagged_tokens = pos_tag(tokens)\n",
        "  lemmatized_tokens = []\n",
        "  for word, tag in tagged_tokens:\n",
        "    wntag = get_wordnet_pos(tag)\n",
        "    lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=wntag))\n",
        "    return lemmatized_tokens"
      ],
      "metadata": {
        "id": "2x0CSALpDEG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text_rule_based(text):\n",
        "  category_scores = {cat: 0 for cat in keywords}\n",
        "  processed_tokens = preprocess_text(text)\n",
        "  processed_text_string = \" \".join(processed_tokens)\n",
        "  text_lower = text.lower()\n",
        "  for category, kws in keywords.items():\n",
        "    for keyword in kws:\n",
        "      lemmatized_keyword_tokens = preprocess_text(keyword)\n",
        "      if lemmatized_keyword_tokens:\n",
        "        lemmatized_keyword = lemmatized_keyword_tokens[0]\n",
        "        if lemmatized_keyword in processed_tokens:\n",
        "          category_scores[category] += processed_tokens.count(lemmatized_keyword)\n",
        "\n",
        "  for category, phrases in strong_phrases.items():\n",
        "    for phrase in phrases:\n",
        "      if phrase in text_lower:\n",
        "        category_scores[category] += 5 * text_lower.count(phrase)\n",
        "\n",
        "  if \"bank\" in text_lower:\n",
        "    if \"river\" in text_lower or \"tree\" in text_lower or \"road\" in text_lower:\n",
        "      category_scores[\"finance\"] -= 3\n",
        "\n",
        "  if \"chip\" in text_lower:\n",
        "    if \"processor\" in text_lower or \"semiconductor\" in text_lower or \"silicon\" in text_lower:\n",
        "      category_scores[\"technology\"] += 5\n",
        "      category_scores[\"medical\"] -= 1\n",
        "    elif \"implant\" in text_lower or \"medical device\" in text_lower:\n",
        "      category_scores[\"medical\"] += 5\n",
        "      category_scores[\"technology\"] -= 1\n",
        "\n",
        "  if \"model\" in text_lower:\n",
        "    if \"ai\" in text_lower or \"machine learning\" in text_lower or \"data\" in text_lower:\n",
        "      category_scores[\"technology\"] += 3\n",
        "\n",
        "  if \"operations\" in text_lower:\n",
        "    if \"military\" in text_lower or \"combat\" in text_lower or \"troop\" in text_lower:\n",
        "      category_scores[\"military\"] += 3\n",
        "      category_scores[\"finance\"] -= 1\n",
        "      category_scores[\"medical\"] -= 1\n",
        "    elif \"business\" in text_lower or \"financial\" in text_lower or \"supply chain\" in text_lower:\n",
        "      category_scores[\"finance\"] += 3\n",
        "      category_scores[\"military\"] -= 1\n",
        "      category_scores[\"medical\"] -= 1\n",
        "    elif \"surgical\" in text_lower or \"patient\" in text_lower:\n",
        "      category_scores[\"medical\"] += 3\n",
        "      category_scores[\"military\"] -= 1\n",
        "      category_scores[\"finance\"] -= 1\n",
        "\n",
        "  if \"player\" in text_lower:\n",
        "    if \"game console\" in text_lower or \"video game\" in text_lower:\n",
        "      category_scores[\"sports\"] -= 2\n",
        "      category_scores[\"technology\"] += 2\n",
        "\n",
        "  max_score = -1\n",
        "  predicted_category = \"Unclassified\"\n",
        "\n",
        "  for category, score in category_scores.items():\n",
        "      if score > max_score:\n",
        "          max_score = score\n",
        "          predicted_category = category\n",
        "\n",
        "  if max_score > 0:\n",
        "      return predicted_category\n",
        "  else:\n",
        "      return \"Unclassified\""
      ],
      "metadata": {
        "id": "aaiB5wCEDlV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_and_classify():\n",
        "    print(\"Enter the paragraph or text you want to classify.\")\n",
        "    print(\"Press Enter twice to finish your input.\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    input_lines = []\n",
        "    while True:\n",
        "        try:\n",
        "            line = input()\n",
        "            if not line.strip():\n",
        "                break\n",
        "            input_lines.append(line)\n",
        "        except EOFError:\n",
        "            break\n",
        "    input_text = \"\\n\".join(input_lines)\n",
        "\n",
        "    if not input_text.strip():\n",
        "        print(\"No text entered. Please try again.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nClassifying text...\")\n",
        "    classification = classify_text_rule_based(input_text)\n",
        "    print(f\"Predicted Category: {classification.capitalize()}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        get_input_and_classify()\n",
        "        again = input(\"Classify another text? (yes/no): \").lower().strip()\n",
        "        if again != 'yes':\n",
        "            print(\"Exiting classifier. Goodbye!\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPZ2R6AJFSwY",
        "outputId": "f224ea9b-843a-4413-a46a-4daca056462b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the paragraph or text you want to classify.\n",
            "Press Enter twice to finish your input.\n",
            "------------------------------\n",
            "Financial statements (or financial reports) are formal records of the financial activities and position of a business, person, or other entity. Relevant financial information is presented in a structured manner and in a form which is easy to understand\n",
            "\n",
            "\n",
            "Classifying text...\n",
            "Predicted Category: Finance\n",
            "------------------------------\n",
            "Classify another text? (yes/no): no\n",
            "Exiting classifier. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD4Xe-5lMrsb",
        "outputId": "f164b7de-c18a-427f-e707-8f8afa6d55c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok -q"
      ],
      "metadata": {
        "id": "MgEDmHWCMsXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 30XaS4o0xIVvFbJki0FokewMWxw_45pjqGyT7TvkpTeGR4BNu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuJQYxRwM31p",
        "outputId": "57ff3579-5d5b-40c2-b569-e11197a2d7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "import re\n",
        "\n",
        "# --- NLTK Downloads and Initializations (Cached for Efficiency) ---\n",
        "# Use st.cache_resource to ensure these expensive operations run only once\n",
        "@st.cache_resource\n",
        "def load_nltk_data():\n",
        "    # nltk.download('punkt', quiet=True) # Already downloaded in initial setup\n",
        "    # nltk.download('stopwords', quiet=True)\n",
        "    # nltk.download('wordnet', quiet=True)\n",
        "    # nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "    return WordNetLemmatizer(), set(stopwords.words('english'))\n",
        "\n",
        "lemmatizer, stop_words = load_nltk_data()\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN # Default to noun if no clear POS tag\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text_lower = text.lower()\n",
        "    tokens = word_tokenize(text_lower)\n",
        "    tokens = [word for word in tokens if word not in string.punctuation and word.isalpha()]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    lemmatized_tokens = []\n",
        "    for word, tag in tagged_tokens:\n",
        "        wntag = get_wordnet_pos(tag)\n",
        "        lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=wntag))\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# --- Define Your Categories and Keywords ---\n",
        "keywords = {\n",
        "    \"finance\": [\n",
        "        \"stock\", \"market\", \"investment\", \"economy\", \"bank\", \"loan\", \"interest rate\",\n",
        "        \"inflation\", \"currency\", \"profit\", \"loss\", \"revenue\", \"expenditure\",\n",
        "        \"dividend\", \"portfolio\", \"trade\", \"share\", \"bond\", \"equity\", \"financial\",\n",
        "        \"gdp\", \"recession\", \"fund\", \"asset\", \"liability\", \"tax\", \"budget\", \"audit\",\n",
        "        \"venture capital\", \"ipo\", \"merger\", \"acquisition\", \"debt\", \"credit\", \"fiscal\",\n",
        "        \"earning\", \"forecast\", \"yield\", \"commodity\", \"derivative\", \"hedging\",\n",
        "        \"exchange\", \"liquidity\", \"capital\", \"underwriting\", \"broker\", \"analyst\"\n",
        "    ],\n",
        "    \"medical\": [\n",
        "        \"patient\", \"doctor\", \"hospital\", \"disease\", \"treatment\", \"diagnosis\", \"symptom\",\n",
        "        \"medication\", \"surgery\", \"therapy\", \"clinic\", \"health\", \"illness\", \"virus\",\n",
        "        \"bacteria\", \"vaccine\", \"research\", \"clinical trial\", \"anatomy\", \"physiology\",\n",
        "        \"pharmacology\", \"epidemic\", \"pandemic\", \"prescription\", \"drug\", \"mri\", \"x-ray\",\n",
        "        \"rehabilitation\", \"nursing\", \"medical device\", \"consultation\", \"prevention\",\n",
        "        \"wellness\", \"pathology\", \"immunology\", \"neurology\", \"cardiology\", \"oncology\",\n",
        "        \"pediatrics\", \"geriatrics\", \"diagnosis\", \"prognosis\", \"therapy\", \"biotechnology\",\n",
        "        \"medical record\", \"health care\", \"clinical study\", \"pharma\"\n",
        "    ],\n",
        "    \"technology\": [\n",
        "        \"software\", \"hardware\", \"computer\", \"internet\", \"network\", \"data\", \"ai\",\n",
        "        \"machine learning\", \"algorithm\", \"cybersecurity\", \"development\", \"programming\",\n",
        "        \"code\", \"app\", \"website\", \"cloud computing\", \"startup\", \"innovation\", \"digital\",\n",
        "        \"robotics\", \"automation\", \"gadget\", \"smartphone\", \"blockchain\", \"virtual reality\",\n",
        "        \"augmented reality\", \"chip\", \"processor\", \"server\", \"database\", \"api\", \"ux\", \"ui\",\n",
        "        \"firmware\", \"protocol\", \"bandwidth\", \"encryption\", \"biometric\", \"nanotechnology\",\n",
        "        \"telecom\", \"gigabit\", \"ethernet\", \"analytic\", \"big data\", \"coding\", \"developer\",\n",
        "        \"neural network\", \"artificial intelligence\", \"tech company\", \"operating system\"\n",
        "    ],\n",
        "    \"sports\": [\n",
        "        \"game\", \"match\", \"team\", \"player\", \"score\", \"win\", \"lose\", \"championship\", \"league\",\n",
        "        \"athlete\", \"coach\", \"stadium\", \"tournament\", \"goal\", \"point\", \"record\", \"training\",\n",
        "        \"fitness\", \"olympic\", \"world cup\", \"basketball\", \"football\", \"cricket\", \"tennis\",\n",
        "        \"swimming\", \"runner\", \"umpire\", \"referee\", \"medal\", \"cup\", \"season\", \"pitch\", \"court\",\n",
        "        \"field\", \"race\", \"compete\", \"victory\", \"defeat\", \"champion\", \"trophy\", \"fixture\",\n",
        "        \"event\", \"fan\", \"supporter\", \"league table\", \"transfer\"\n",
        "    ],\n",
        "    \"military\": [\n",
        "        \"army\", \"navy\", \"air force\", \"soldier\", \"weapon\", \"combat\", \"war\", \"defense\",\n",
        "        \"security\", \"military operation\", \"troop\", \"battle\", \"intelligence\", \"commander\",\n",
        "        \"strategy\", \"tactics\", \"deployment\", \"veteran\", \"peacekeeping\", \"ammunition\",\n",
        "        \"artillery\", \"infantry\", \"navy seal\", \"air strike\", \"missile\", \"uniform\", \"drill\",\n",
        "        \"sanction\", \"bunker\", \"grenade\", \"camouflage\", \"reconnaissance\", \"logistics\",\n",
        "        \"fortification\", \"cadet\", \"serviceman\", \"patrol\", \"conflict\", \"frontline\",\n",
        "        \"regiment\", \"battalion\", \"squadron\", \"fleet\", \"armored\", \"counter-terrorism\",\n",
        "        \"military base\", \"armed force\", \"national guard\", \"servicemember\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "strong_phrases = {\n",
        "    \"finance\": [\"stock market\", \"interest rate hike\", \"quarterly earnings\", \"financial crisis\", \"venture capital\", \"fiscal policy\", \"monetary policy\", \"investment bank\", \"credit rating\", \"economic growth\"],\n",
        "    \"medical\": [\"clinical trial results\", \"public health emergency\", \"disease outbreak\", \"medical breakthrough\", \"patient care\", \"emergency room\", \"side effects\", \"health care system\", \"surgical procedure\", \"drug discovery\"],\n",
        "    \"technology\": [\"artificial intelligence\", \"machine learning\", \"cyber security threat\", \"software development\", \"cloud computing\", \"virtual reality\", \"augmented reality\", \"data science\", \"internet of things\", \"quantum computing\", \"neural network\"],\n",
        "    \"sports\": [\"world cup final\", \"championship game\", \"team victory\", \"olympic medal\", \"record breaking\", \"league leader\", \"playoff series\", \"final score\", \"athlete performance\", \"sporting event\"],\n",
        "    \"military\": [\"military exercise\", \"national security\", \"combat operations\", \"defense budget\", \"peacekeeping mission\", \"armed forces\", \"intelligence gathering\", \"strategic deployment\", \"air superiority\", \"military alliance\", \"special forces\"]\n",
        "}\n",
        "\n",
        "# --- Classification Function ---\n",
        "def classify_text_rule_based(text):\n",
        "    if not text:\n",
        "        return \"Please enter some text for classification.\"\n",
        "\n",
        "    processed_tokens = preprocess_text(text)\n",
        "    # processed_text_string = \" \".join(processed_tokens) # Not strictly needed\n",
        "\n",
        "    category_scores = {cat: 0 for cat in keywords}\n",
        "\n",
        "    # Debugging prints (commented out for cleaner Streamlit output)\n",
        "    # st.write(f\"Processed Tokens: {processed_tokens}\")\n",
        "    # st.write(\"--- Rule 1: Individual Keyword Counts ---\")\n",
        "\n",
        "    # Rule 1: Count individual keyword occurrences in lemmatized tokens\n",
        "    for category, kws in keywords.items():\n",
        "        for keyword in kws:\n",
        "            lemmatized_keyword_tokens = preprocess_text(keyword)\n",
        "            if lemmatized_keyword_tokens:\n",
        "                lemmatized_keyword = lemmatized_keyword_tokens[0]\n",
        "                if lemmatized_keyword in processed_tokens:\n",
        "                    count = processed_tokens.count(lemmatized_keyword)\n",
        "                    category_scores[category] += count\n",
        "                    # st.write(f\"  Found '{lemmatized_keyword}' ({count}x) for {category.capitalize()}\")\n",
        "    # st.write(f\"Scores after Rule 1: {category_scores}\")\n",
        "\n",
        "    # Rule 2: Score for strong phrases (matched in original lowercased text)\n",
        "    # st.write(\"--- Rule 2: Strong Phrase Matches ---\")\n",
        "    text_lower = text.lower() # Use original lowercased text for phrase matching\n",
        "    for category, phrases in strong_phrases.items():\n",
        "        for phrase in phrases:\n",
        "            if phrase in text_lower:\n",
        "                count = text_lower.count(phrase)\n",
        "                category_scores[category] += 5 * count\n",
        "                # st.write(f\"  Found strong phrase '{phrase}' ({count}x, +{5*count} pts) for {category.capitalize()}\")\n",
        "    # st.write(f\"Scores after Rule 2 (cumulative): {category_scores}\")\n",
        "\n",
        "    # Rule 3: Handle potential ambiguities/exclusions\n",
        "    # st.write(\"--- Rule 3: Ambiguity/Exclusion Adjustments ---\")\n",
        "    # Initial scores before ambiguity for debugging (optional)\n",
        "    # initial_scores_before_ambiguity = category_scores.copy()\n",
        "\n",
        "    if \"bank\" in text_lower:\n",
        "        if \"river\" in text_lower or \"tree\" in text_lower or \"road\" in text_lower:\n",
        "            category_scores[\"finance\"] -= 3\n",
        "            # st.write(f\"  Adjusting Finance for 'bank' context: -3. New Score: {category_scores['finance']}\")\n",
        "\n",
        "    if \"chip\" in text_lower:\n",
        "        if \"processor\" in text_lower or \"semiconductor\" in text_lower or \"silicon\" in text_lower:\n",
        "            category_scores[\"technology\"] += 5\n",
        "            category_scores[\"medical\"] -= 1\n",
        "            # st.write(f\"  Adjusting for 'chip' (tech context): Tech +5, Medical -1\")\n",
        "        elif \"implant\" in text_lower or \"medical device\" in text_lower:\n",
        "            category_scores[\"medical\"] += 5\n",
        "            category_scores[\"technology\"] -= 1\n",
        "            # st.write(f\"  Adjusting for 'chip' (medical context): Medical +5, Tech -1\")\n",
        "        elif \"potato\" in text_lower or \"snack\" in text_lower:\n",
        "             category_scores[\"technology\"] -= 2\n",
        "             category_scores[\"medical\"] -= 2\n",
        "             category_scores[\"sports\"] -= 1\n",
        "             # st.write(f\"  Adjusting for 'chip' (food context): Tech -2, Medical -2, Sports -1\")\n",
        "\n",
        "    if \"model\" in text_lower:\n",
        "        if \"ai\" in text_lower or \"machine learning\" in text_lower or \"data\" in text_lower:\n",
        "            category_scores[\"technology\"] += 3\n",
        "            # st.write(f\"  Adjusting for 'model' (AI context): Tech +3\")\n",
        "\n",
        "    if \"operations\" in text_lower:\n",
        "        if \"military\" in text_lower or \"combat\" in text_lower or \"troop\" in text_lower:\n",
        "            category_scores[\"military\"] += 3\n",
        "            category_scores[\"finance\"] -= 1\n",
        "            category_scores[\"medical\"] -= 1\n",
        "            # st.write(f\"  Adjusting for 'operations' (military context): Military +3, Finance -1, Medical -1\")\n",
        "        elif \"business\" in text_lower or \"financial\" in text_lower or \"supply chain\" in text_lower:\n",
        "            category_scores[\"finance\"] += 3\n",
        "            category_scores[\"military\"] -= 1\n",
        "            category_scores[\"medical\"] -= 1\n",
        "            # st.write(f\"  Adjusting for 'operations' (business context): Finance +3, Military -1, Medical -1\")\n",
        "        elif \"surgical\" in text_lower or \"patient\" in text_lower:\n",
        "            category_scores[\"medical\"] += 3\n",
        "            category_scores[\"military\"] -= 1\n",
        "            category_scores[\"finance\"] -= 1\n",
        "            # st.write(f\"  Adjusting for 'operations' (medical context): Medical +3, Military -1, Finance -1\")\n",
        "\n",
        "    if \"player\" in text_lower:\n",
        "        if \"game console\" in text_lower or \"video game\" in text_lower:\n",
        "            category_scores[\"sports\"] -= 2\n",
        "            category_scores[\"technology\"] += 2\n",
        "            # st.write(f\"  Adjusting for 'player' (gaming context): Sports -2, Tech +2\")\n",
        "\n",
        "    if \"drone technology\" in text_lower:\n",
        "        category_scores[\"military\"] += 4\n",
        "        category_scores[\"technology\"] += 2\n",
        "        # st.write(f\"  Adjusting for 'drone technology': Military +4, Tech +2\")\n",
        "\n",
        "    # st.write(f\"Scores after Rule 3 (Final adjustments): {category_scores}\")\n",
        "\n",
        "    # Determine the winning category\n",
        "    max_score = -1\n",
        "    predicted_category = \"Unclassified\"\n",
        "\n",
        "    for category, score in category_scores.items():\n",
        "        if score > max_score:\n",
        "            max_score = score\n",
        "            predicted_category = category\n",
        "\n",
        "    if max_score > 0:\n",
        "        # st.write(f\"\\nFinal Max Score: {max_score} for {predicted_category.capitalize()}\")\n",
        "        return predicted_category.capitalize() # Capitalize directly here for output consistency\n",
        "    else:\n",
        "        # st.write(\"\\nFinal Max Score: 0. Returning Unclassified.\")\n",
        "        return \"Unclassified\"\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(page_title=\"Text Classifier\", layout=\"centered\")\n",
        "\n",
        "st.title(\"💡 Rule-Based Text Classifier 🚀\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "Welcome to the Rule-Based Text Classifier!\n",
        "Enter a paragraph or any piece of text below, and our AI will attempt to classify it into one of these categories:\n",
        "**Finance, Medical, Technology, Sports, or Military.**\n",
        "\"\"\")\n",
        "\n",
        "text_input = st.text_area(\n",
        "    \"Enter your text here:\",\n",
        "    height=200,\n",
        "    placeholder=\"Example: 'The new AI model improved stock market predictions, leading to significant financial gains. Meanwhile, the basketball team won their championship game.'\"\n",
        ")\n",
        "\n",
        "if st.button(\"Classify Text\"):\n",
        "    if text_input:\n",
        "        with st.spinner(\"Classifying...\"):\n",
        "            result = classify_text_rule_based(text_input)\n",
        "            st.success(f\"**Predicted Category: {result}**\")\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"Category Scores (for debugging/insight):\")\n",
        "            # To show scores, you'd need to modify classify_text_rule_based to return scores too\n",
        "            # For now, let's just show a placeholder or adapt the function to return it.\n",
        "            # A simpler way for a demo is to re-calculate, but that's inefficient.\n",
        "            # Best practice: make classify_text_rule_based return (category, scores_dict)\n",
        "\n",
        "            # For demonstration, let's just re-run parts of the logic to get scores for display.\n",
        "            # In a real app, modify classify_text_rule_based to return the scores dict as well.\n",
        "            temp_processed_tokens = preprocess_text(text_input)\n",
        "            temp_category_scores = {cat: 0 for cat in keywords}\n",
        "\n",
        "            for category, kws in keywords.items():\n",
        "                for keyword in kws:\n",
        "                    lemmatized_keyword_tokens = preprocess_text(keyword)\n",
        "                    if lemmatized_keyword_tokens:\n",
        "                        lemmatized_keyword = lemmatized_keyword_tokens[0]\n",
        "                        if lemmatized_keyword in temp_processed_tokens:\n",
        "                            temp_category_scores[category] += temp_processed_tokens.count(lemmatized_keyword)\n",
        "\n",
        "            text_lower_temp = text_input.lower()\n",
        "            for category, phrases in strong_phrases.items():\n",
        "                for phrase in phrases:\n",
        "                    if phrase in text_lower_temp:\n",
        "                        temp_category_scores[category] += 5 * text_lower_temp.count(phrase)\n",
        "\n",
        "            # Re-apply ambiguity rules for display\n",
        "            if \"bank\" in text_lower_temp:\n",
        "                if \"river\" in text_lower_temp or \"tree\" in text_lower_temp or \"road\" in text_lower_temp:\n",
        "                    temp_category_scores[\"finance\"] -= 3\n",
        "            if \"chip\" in text_lower_temp:\n",
        "                if \"processor\" in text_lower_temp or \"semiconductor\" in text_lower_temp or \"silicon\" in text_lower_temp:\n",
        "                    temp_category_scores[\"technology\"] += 5\n",
        "                    temp_category_scores[\"medical\"] -= 1\n",
        "                elif \"implant\" in text_lower_temp or \"medical device\" in text_lower_temp:\n",
        "                    temp_category_scores[\"medical\"] += 5\n",
        "                    temp_category_scores[\"technology\"] -= 1\n",
        "                elif \"potato\" in text_lower_temp or \"snack\" in text_lower_temp:\n",
        "                    temp_category_scores[\"technology\"] -= 2\n",
        "                    temp_category_scores[\"medical\"] -= 2\n",
        "                    temp_category_scores[\"sports\"] -= 1\n",
        "            if \"model\" in text_lower_temp:\n",
        "                if \"ai\" in text_lower_temp or \"machine learning\" in text_lower_temp or \"data\" in text_lower_temp:\n",
        "                    temp_category_scores[\"technology\"] += 3\n",
        "            if \"operations\" in text_lower_temp:\n",
        "                if \"military\" in text_lower_temp or \"combat\" in text_lower_temp or \"troop\" in text_lower_temp:\n",
        "                    temp_category_scores[\"military\"] += 3\n",
        "                    temp_category_scores[\"finance\"] -= 1\n",
        "                    temp_category_scores[\"medical\"] -= 1\n",
        "                elif \"business\" in text_lower_temp or \"financial\" in text_lower_temp or \"supply chain\" in text_lower_temp:\n",
        "                    temp_category_scores[\"finance\"] += 3\n",
        "                    temp_category_scores[\"military\"] -= 1\n",
        "                    temp_category_scores[\"medical\"] -= 1\n",
        "                elif \"surgical\" in text_lower_temp or \"patient\" in text_lower_temp:\n",
        "                    temp_category_scores[\"medical\"] += 3\n",
        "                    temp_category_scores[\"military\"] -= 1\n",
        "                    temp_category_scores[\"finance\"] -= 1\n",
        "            if \"player\" in text_lower_temp:\n",
        "                if \"game console\" in text_lower_temp or \"video game\" in text_lower_temp:\n",
        "                    temp_category_scores[\"sports\"] -= 2\n",
        "                    temp_category_scores[\"technology\"] += 2\n",
        "            if \"drone technology\" in text_lower_temp:\n",
        "                temp_category_scores[\"military\"] += 4\n",
        "                temp_category_scores[\"technology\"] += 2\n",
        "\n",
        "            # Sort scores for better display\n",
        "            sorted_scores = sorted(temp_category_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "            for cat, score in sorted_scores:\n",
        "                st.write(f\"- **{cat.capitalize()}:** {score} points\")\n",
        "\n",
        "    else:\n",
        "        st.warning(\"Please enter some text to classify.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"Built with ❤️ using Streamlit & NLTK\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_RWFCfuOLA5",
        "outputId": "d82a3731-c8c6-460f-ec68-cdfbfd61537d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Kill any existing ngrok processes (useful if you restart cells multiple times)\n",
        "!kill -9 $(lsof -t -i:8501) >/dev/null 2>&1 || true\n",
        "!kill -9 $(lsof -t -i:4040) >/dev/null 2>&1 || true # For ngrok's own UI\n",
        "\n",
        "# Start Streamlit app in the background\n",
        "print(\"Starting Streamlit app in the background...\")\n",
        "# Use nohup to detach the process, & to run in background\n",
        "# This command prevents Streamlit from blocking the Colab cell\n",
        "process = subprocess.Popen(['nohup', 'streamlit', 'run', 'app.py', '--server.port', '8501', '--server.enableCORS', 'false', '--server.enableXsrfProtection', 'false'],\n",
        "                           stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "# Give Streamlit a moment to start\n",
        "import time\n",
        "time.sleep(5)\n",
        "\n",
        "# Connect ngrok to Streamlit's port 8501\n",
        "print(\"Connecting ngrok tunnel...\")\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Your Streamlit app is live at: {public_url}\")\n",
        "\n",
        "# Optional: Print Streamlit's server output (for debugging if app doesn't load)\n",
        "# You might see logs like \"You can now view your Streamlit app in your browser.\"\n",
        "# and connection messages from ngrok.\n",
        "# If you don't see the app, comment out time.sleep(5) and try to read output more carefully\n",
        "# for line in iter(process.stdout.readline, ''):\n",
        "#     print(line, end='')\n",
        "#     if \"You can now view your Streamlit app in your browser.\" in line:\n",
        "#         break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSSzxppXOQv5",
        "outputId": "b1276203-4263-4d3e-9b12-bca4f263fc88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit app in the background...\n",
            "Connecting ngrok tunnel...\n",
            "Your Streamlit app is live at: NgrokTunnel: \"https://b6fce3def7ef.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k4prKbQwPRkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}